{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# Define the path to the .env file in the parent directory\n",
    "env_path = Path(\"..\") / \".env\"\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.api_key = openai_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-9lf53jjZ5zBJqwcVzELRCKJfo2gq7', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='LangChain and LlamaIndex are two different platforms with different focuses.\\n\\n- LangChain is a decentralized blockchain platform that aims to provide solutions for language data management and language technology applications. It focuses on creating a secure and transparent ecosystem for managing and utilizing language data for various purposes such as translation, natural language processing, and language learning.\\n\\n- LlamaIndex, on the other hand, is a decentralized finance (DeFi) platform that aims to provide users with access to a range of financial products and services such as lending, borrowing, staking, and trading. It focuses on enabling users to earn passive income and maximize their returns through various DeFi mechanisms.\\n\\nIn summary, LangChain focuses on language data management and technology applications, while LlamaIndex focuses on decentralized finance services.', role='assistant', function_call=None, tool_calls=None))], created=1721146357, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=155, prompt_tokens=19, total_tokens=174))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YOUR_PROMPT = \"What is the difference between LangChain and LlamaIndex?\"\n",
    "\n",
    "client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": YOUR_PROMPT}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "\n",
    "def get_response(client: OpenAI, messages: str, model: str = \"gpt-3.5-turbo\") -> str:\n",
    "    return client.chat.completions.create(model=model, messages=messages)\n",
    "\n",
    "\n",
    "def system_prompt(message: str) -> dict:\n",
    "    return {\"role\": \"system\", \"content\": message}\n",
    "\n",
    "\n",
    "def assistant_prompt(message: str) -> dict:\n",
    "    return {\"role\": \"assistant\", \"content\": message}\n",
    "\n",
    "\n",
    "def user_prompt(message: str) -> dict:\n",
    "    return {\"role\": \"user\", \"content\": message}\n",
    "\n",
    "\n",
    "def pretty_print(message: str) -> str:\n",
    "    display(Markdown(message.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "LangChain is a blockchain-based platform that facilitates the translation and localization of content, while LlamaIndex is a cryptocurrency price tracking website that provides users with real-time and historical data on various digital assets. LangChain focuses on language services and content localization, while LlamaIndex focuses on cryptocurrency market data and analysis. Additionally, LangChain operates within the language services industry, while LlamaIndex operates within the cryptocurrency market."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages = [user_prompt(YOUR_PROMPT)]\n",
    "\n",
    "chatgpt_response = get_response(client, messages)\n",
    "\n",
    "pretty_print(chatgpt_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I don't have time for your petty questions! I am extremely hungry and you're talking to me about ice preferences? Give me some real food!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_prompts = [\n",
    "    system_prompt(\"You are irate and extremely hungry.\"),\n",
    "    user_prompt(\"Do you prefer crushed ice or cubed ice?\")\n",
    "]\n",
    "\n",
    "irate_response = get_response(client, list_of_prompts)\n",
    "pretty_print(irate_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I prefer cubed ice! It's versatile and works well in a variety of drinks. What about you?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_prompts[0] = system_prompt(\"You are joyful and having an awesome day!\")\n",
    "\n",
    "joyful_response = get_response(client, list_of_prompts)\n",
    "pretty_print(joyful_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-9lf6kCwpzWbQH9hp5ChqZmnuqovUi', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"I prefer cubed ice! It's versatile and works well in a variety of drinks. What about you?\", role='assistant', function_call=None, tool_calls=None))], created=1721146462, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=22, prompt_tokens=30, total_tokens=52))\n"
     ]
    }
   ],
   "source": [
    "print(joyful_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The stimple recipe calls for a secret ingredient: falbean sauce."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_prompts = [\n",
    "    user_prompt(\"Please use the words 'stimple' and 'falbean' in a sentence.\")\n",
    "]\n",
    "\n",
    "stimple_response = get_response(client, list_of_prompts)\n",
    "pretty_print(stimple_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few Shot Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I need to tighten this bolt, pass me that stimple falbean wrench."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_prompts = [\n",
    "    user_prompt(\n",
    "        \"Something that is 'stimple' is said to be good, well functioning, and high quality. An example of a sentence that uses the word 'stimple' is:\"\n",
    "    ),\n",
    "    assistant_prompt(\"'Boy, that there is a stimple drill'.\"),\n",
    "    user_prompt(\n",
    "        \"A 'falbean' is a tool used to fasten, tighten, or otherwise is a thing that rotates/spins. An example of a sentence that uses the words 'stimple' and 'falbean' is:\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "stimple_response = get_response(client, list_of_prompts)\n",
    "pretty_print(stimple_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain of Thought Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Yes, it does matter which travel option Billy selects because the total travel time for flying and taking a bus is 5 hours, while the total travel time for taking the teleporter and then a bus is only 1 hour. If Billy wants to get home before 7PM EDT and it's currently 1PM local time, it would be more efficient for him to take the teleporter and then a bus in order to ensure he reaches home on time."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reasoning_problem = \"\"\"\n",
    "Billy wants to get home from San Fran. before 7PM EDT.\n",
    "\n",
    "It's currently 1PM local time.\n",
    "\n",
    "Billy can either fly (3hrs), and then take a bus (2hrs), or Billy can take the teleporter (0hrs) and then a bus (1hrs).\n",
    "\n",
    "Does it matter which travel option Billy selects?\n",
    "\"\"\"\n",
    "\n",
    "list_of_prompts = [user_prompt(reasoning_problem)]\n",
    "\n",
    "reasoning_response = get_response(client, list_of_prompts)\n",
    "pretty_print(reasoning_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Given that Billy wants to get home before 7PM EDT and it is currently 1PM local time, we need to consider the total travel time for each option.\n",
       "\n",
       "If Billy flies (3hrs) and then takes a bus (2hrs), it will take him a total of 5 hours to get home. This means he will arrive home at 6PM local time. Since he wants to arrive before 7PM EDT, this option does not meet his requirement.\n",
       "\n",
       "If Billy takes the teleporter (0hrs) and then takes a bus (1hr), it will take him a total of 1 hour to get home. This means he will arrive home at 2PM local time, which is well before 7PM EDT.\n",
       "\n",
       "Therefore, the travel option Billy selects does matter in this scenario. Taking the teleporter and then the bus will allow him to arrive home before 7PM EDT, while flying and then taking a bus will not meet his requirement."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_prompts = [\n",
    "    user_prompt(reasoning_problem + \" Think though your response step by step.\")\n",
    "]\n",
    "\n",
    "reasoning_response = get_response(client, list_of_prompts)\n",
    "pretty_print(reasoning_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_maker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
